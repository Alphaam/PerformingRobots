When doing this week's reading, a question that kept coming up for me was "what is the emotional baseline used when having conversations about robots and human emotions?" A lot was written about the value of robots being able to convey or ilicit emotions from an audience. Whenever this was brought up, it did seem like the author was refering to some known emotional baseline that is universally considered "happy" or "sad." However, emotions, as I understand them atleast, do not work like that. There is no universal "happy" or "sad" because we are all wired so differently and feel so differently based on biology, psychology, environemnet, etc. I wonder what would happen to this conversation about robots and human emotions when we attempt to do a way with this implied base-line.
